{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "**Word embeddings** are vector representations of words that capture semantic information and relationships between them. The idea is to transform words into numerical vectors where words with similar meanings or contexts have similar representations. This approach allows machine learning models to understand language better by placing words in a high-dimensional space where distance indicates similarity.\n"
      ],
      "metadata": {
        "id": "6_drgPwx4gRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xonQtTza4Ge3"
      },
      "outputs": [],
      "source": []
    }
  ]
}