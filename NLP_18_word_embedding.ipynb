{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "**Word embeddings** are vector representations of words that capture semantic information and relationships between them. The idea is to transform words into numerical vectors where words with similar meanings or contexts have similar representations. This approach allows machine learning models to understand language better by placing words in a high-dimensional space where distance indicates similarity.\n"
      ],
      "metadata": {
        "id": "6_drgPwx4gRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Examples of word embedding techniques include:\n",
        "1. **Word2Vec:** This model, developed by Google, learns word embeddings using neural networks. It has two main approaches: **CBOW (Continuous Bag of Words)** and **Skip-Gram.**\n",
        "  * **CBOW** predicts a word based on its surrounding context.\n",
        "  * **Skip-Gram** predicts the context based on a given word.\n",
        "2. **GloVe (Global Vectors for Word Representation):** Developed by Stanford, GloVe captures global statistical information from a corpus by training on word co-occurrence probabilities.\n",
        "3. **FastText:** An extension of Word2Vec developed by Facebook, FastText represents words as n-grams of characters, making it effective for morphologically rich languages and out-of-vocabulary words."
      ],
      "metadata": {
        "id": "7nfHwfjB5Iq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xonQtTza4Ge3"
      },
      "outputs": [],
      "source": []
    }
  ]
}