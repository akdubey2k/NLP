{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "**Word embeddings** are *vector representations of words,* that capture semantic information and relationships between them. The idea is to transform words into numerical vectors where words with similar meanings or contexts have similar representations. This approach allows machine learning models to understand language better by placing words in a high-dimensional space where distance indicates similarity.\n"
      ],
      "metadata": {
        "id": "6_drgPwx4gRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Examples of word embedding techniques include:\n",
        "1. **Word2Vec:** This model, developed by *Google*, learns word embeddings using neural networks. It has two main approaches: **CBOW (Continuous Bag of Words)** and **Skip-Gram.**\n",
        "  * **CBOW,** *predicts a word based on its surrounding context.*\n",
        "  * **Skip-Gram,** *predicts the context based on a given word.*\n",
        "2. **GloVe (Global Vectors for Word Representation):** Developed by *Stanford*, GloVe captures global statistical information from a corpus by training on word co-occurrence probabilities.\n",
        "3. **FastText:** An extension of **Word2Vec** developed by *Facebook*, **FastText** represents words as **n-grams of characters,** making it effective for morphologically (*similar in context*) rich languages and out-of-vocabulary words."
      ],
      "metadata": {
        "id": "7nfHwfjB5Iq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries\n",
        "This imports the `gensim` library and the `Word2Vec` class.\n",
        "* `gensim` is a Python library for training word embeddings and performing **NLP** tasks.\n",
        "* The `Word2Vec` class specifically implements the `Word2Vec` model, which converts words into vector representations."
      ],
      "metadata": {
        "id": "1pQ-Ekyq8cy4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xonQtTza4Ge3"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "# from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare the text dataset\n",
        "* Define a `sentences`, which is a list of lists.\n",
        "* Each inner list is a sentence, split into individual words. These words will be used as **tokens** by the **Word2Vec** model.\n",
        "* In **Word2Vec**, sentences are used to learn the relationships between words, with similar words or words in similar contexts receiving similar vector representations.\n"
      ],
      "metadata": {
        "id": "BJoQ1W625uNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences\n",
        "sentences = [\n",
        "    ['hello', 'world'],\n",
        "    ['i', 'love', 'natural', 'language', 'processing'],\n",
        "    ['hello', 'from', 'the', 'other', 'side']\n",
        "]"
      ],
      "metadata": {
        "id": "s4T_WTG97gre"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initialize and Train Word2Vec Model\n",
        "#### Key parameters:-\n",
        "- **sentences:** *corpus of text, an iterable of sentences, where each sentence is a list of words.*\n",
        "- **vector_size:** *the size of the word vectors.*\n",
        "- **window:** *context window size.*\n",
        "- **min_count:** *minimum frequency of words to be considered.*\n",
        "- **sg:** *training algorithm; sg=1, skip-gram and sg=0, CBOW (Continuous Bag of Words) model.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJrLAEeI8lrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector_size=50; the dimension of the word vectors (embedding size)\n",
        "# window = 3; The maximum distance between the current and predicted word in a sentence. For example, if window=3, Word2Vec will consider up to three words before and after the target word in each context.\n",
        "# min_count=1; Ignores all words with a total frequency lower than this. In this case, all words with a frequency of 1 or more will be considered.\n",
        "# sg=1; Specifies the training algorithm. sg=1 means using the skip-gram model, which tries to predict surrounding words given a center word. If sg=0, Word2Vec uses the CBOW (Continuous Bag of Words) model, which predicts the center word from surrounding context words.\n",
        "model = Word2Vec(sentences, vector_size = 50, window = 3, min_count=1, sg = 1)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoWdJv_QTYyx",
        "outputId": "f09f7ad6-0d1d-4fa9-ffb4-615426b0ddbf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7a10d70ccca0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Accessing Word Vectors\n",
        "- **model.wv** provides access to the trained word vectors.\n",
        "- *it is a 50-dimensional vector space,* capturing semantic information based on the surrounding words it appears with in the training data."
      ],
      "metadata": {
        "id": "z2fEr6F1yyE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector = model.wv['hello']  # Retrieve the vector for the word 'hello'\n",
        "# Prints the vector for the word 'hello'. The output will be a 50-dimensional array of floating-point numbers that represents the word in semantic space.\n",
        "print(word_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yfqkv3iz2KQ",
        "outputId": "fdd6ae87-b0a9-41c8-895a-38ccb648b2ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
            " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
            " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
            " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
            "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
            "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
            "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
            " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
            "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
            "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
            " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
            " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
            "  9.9641159e-03  1.8466286e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding most similar words to \"hello\"\n",
        "similar_words = model.wv.most_similar('hello')\n",
        "# print(similar_words)\n",
        "similar_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdjNCTQ11bnP",
        "outputId": "f5805c07-5971-40b2-91c4-b307e9a889e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('natural', 0.13204392790794373),\n",
              " ('other', 0.1267007291316986),\n",
              " ('i', 0.09984847903251648),\n",
              " ('side', 0.042373016476631165),\n",
              " ('processing', 0.012442179024219513),\n",
              " ('world', -0.012591075152158737),\n",
              " ('the', -0.01447527389973402),\n",
              " ('love', -0.0560765340924263),\n",
              " ('language', -0.05974648892879486),\n",
              " ('from', -0.11821284145116806)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ]
}