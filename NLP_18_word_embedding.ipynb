{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "**Word embeddings** are *vector representations of words,* that capture semantic information and relationships between them. The idea is to transform words into numerical vectors where words with similar meanings or contexts have similar representations. This approach allows machine learning models to understand language better by placing words in a high-dimensional space where distance indicates similarity.\n"
      ],
      "metadata": {
        "id": "6_drgPwx4gRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Examples of word embedding techniques include:\n",
        "1. **Word2Vec:** This model, developed by *Google*, learns word embeddings using neural networks. It has two main approaches: **CBOW (Continuous Bag of Words)** and **Skip-Gram.**\n",
        "  * **CBOW** predicts a word based on its surrounding context.\n",
        "  * **Skip-Gram** predicts the context based on a given word.\n",
        "2. **GloVe (Global Vectors for Word Representation):** Developed by *Stanford*, GloVe captures global statistical information from a corpus by training on word co-occurrence probabilities.\n",
        "3. **FastText:** An extension of Word2Vec developed by *Facebook*, **FastText** represents words as n-grams of characters, making it effective for morphologically (*similar in context*) rich languages and out-of-vocabulary words."
      ],
      "metadata": {
        "id": "7nfHwfjB5Iq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries\n",
        "This imports the `gensim` library and the `Word2Vec` class.\n",
        "* `gensim` is a Python library for training word embeddings and performing **NLP** tasks.\n",
        "* The `Word2Vec` class specifically implements the `Word2Vec` model, which converts words into vector representations."
      ],
      "metadata": {
        "id": "1pQ-Ekyq8cy4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xonQtTza4Ge3"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prepare the text dataset\n",
        "* Define a `sentences`, which is a list of lists.\n",
        "* Each inner list is a sentence, split into individual words. These words will be used as **tokens** by the **Word2Vec** model.\n",
        "* In **Word2Vec**, sentences are used to learn the relationships between words, with similar words or words in similar contexts receiving similar vector representations.\n"
      ],
      "metadata": {
        "id": "BJoQ1W625uNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences\n",
        "sentences = [\n",
        "    ['hello', 'world'],\n",
        "    ['i', 'love', 'natural', 'language', 'processing'],\n",
        "    ['hello', 'from', 'the', 'other', 'side']\n",
        "]"
      ],
      "metadata": {
        "id": "s4T_WTG97gre"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Initialize and Train Word2Vec Model\n",
        "#### Key parameters:-\n",
        "- sentences: *corpus of text, an iterable of sentences, where each sentence is a list of words.*\n",
        "- vector_size: *the size of the word vectors.*\n",
        "- window: *context window size.*\n",
        "- min_count: *minimum frequency of words to be considered.*\n",
        "- sg: *training algorithm; sg=1, skip-gram and sg=0, CBOW (Continuous Bag of Words) model.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YJrLAEeI8lrd"
      }
    }
  ]
}