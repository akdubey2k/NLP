{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Stock Sentiment Analysis Using Financial News\n",
        "Performing sentiment analysis on Indian stock market news using financial news from reputed sources like **Economic Times, Financial Express, or Bloomberg Quint** involves several steps. The goal is to process news headlines or articles, extract sentiment, and analyze the impact on stock prices.\n",
        "\n",
        "## 1.2 Steps Involved\n",
        "1. **Data Collection:** Collect financial news data (headlines/articles) from reliable sources using APIs, web scraping, or publicly available datasets.\n",
        "2. **Preprocessing:** Clean and preprocess the text data.\n",
        "3. **Sentiment Analysis:** Use natural language processing (NLP) models to analyze the sentiment (positive, negative, or neutral) of the news.\n",
        "4. **Stock Analysis:** Correlate the sentiment with stock market trends (e.g., stock price changes)."
      ],
      "metadata": {
        "id": "ogaZKvWymOjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Implementation Requirements\n",
        "### Libraries:\n",
        "* `pandas, nltk, transformers, beautifulsoup4, requests` (for scraping), or APIs for data gathering.\n",
        "* **Sentiment analysis models:** Traditional models like **VADER** or advanced models like **BERT** for financial sentiment."
      ],
      "metadata": {
        "id": "9af0q18BnW2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries\n",
        "required these libraries when scraping data from websites and convert the extracted content into a structured format from webpages, like stock news, blogs, articles, etc.\n"
      ],
      "metadata": {
        "id": "wgwqR_kJoWR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for sending HTTP requests; it allows us to access and retrieve data from the web\n",
        "import requests\n",
        "\n",
        "# for parsing HTML; it helps us parse and navigate HTML, making it easy to extract data like news headlines.\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# for data manipulation; it allows us to organize and manipulate this data in tabular form\n",
        "import pandas as pd\n",
        "\n",
        "# Comprehensive library for natural language processing tasks, where text data needs to be cleaned, tokenized, and transformed.\n",
        "import nltk\n",
        "\n",
        "# helps to remove stopwords like 'and', 'the', 'is', which do not contribute much to sentiment\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# breaking text into individual words (tokenization).\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Regular expression operations from the 're' module, to check if a string contains the specified search pattern.\n",
        "# https://regex101.com/\n",
        "import re"
      ],
      "metadata": {
        "id": "eccPH_KNoadF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Collection\n",
        "## Step 1: Data collection with web scraping\n",
        "* Collect financial news data (scraping from **Economic Times** as an example)\n",
        "* sends an HTTP GET request to the 'Economic Times' website to get the page’s HTML content and parses it using `BeautifulSoup`."
      ],
      "metadata": {
        "id": "PIy2L6iCqpAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://economictimes.indiatimes.com/markets/stocks\"\n",
        "\n",
        "# sends an HTTP request to the 'Economic Times' website to get the page’s HTML content and retrieves the webpage’s HTML.\n",
        "response = requests.get(url)\n",
        "\n",
        "# response is then passed to 'BeautifulSoup' for parsing so that we can access individual HTML elements, such as the headlines.\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "# soup"
      ],
      "metadata": {
        "id": "Sy4tIMXBq4UX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* extracts all the **h3** tags from the parsed HTML content\n",
        "* most news websites, headlines are usually wrapped in specific HTML tags like **h3**; extract these to get the actual news headlines.\n",
        "* Use this to extract specific HTML tags that contain the information (e.g., headlines, articles).\n"
      ],
      "metadata": {
        "id": "szf9cpOk0Hy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find and extracts all headline elements (this depends on the website structure)\n",
        "headlines = soup.find_all('h3')  # Modify tag/structure based on source\n",
        "\n",
        "# Extract the text from the headlines\n",
        "news_data = []\n",
        "for headline in headlines:\n",
        "    news_data.append(headline.get_text())\n",
        "# news_data"
      ],
      "metadata": {
        "id": "e1ig4Gd4zePr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data organization using **Pandas** dataframe\n",
        "* converts the list of headlines into a `pandas DataFrame`, with the column name **“Headline”**\n",
        "* used for tabular organization of data to organize, manipulate, or analyze using pandas’ powerful DataFrame functionality.\n"
      ],
      "metadata": {
        "id": "IFwMdL0O2h-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.DataFrame(news_data, columns=[\"Headline\"])\n",
        "print(news_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fwsj7Zx3fhs",
        "outputId": "d371587b-ad49-46c9-f1e1-749933e570ac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Headline\n",
            "0                                         BULL'S EYE\n",
            "1  \\n                            Must Watch\\n    ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preprocessing the Text Data\n",
        "Dataset EDA (Exploratory Data Analysis)"
      ],
      "metadata": {
        "id": "BVxi2AMT82fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords once (if not already installed)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Download punkt tokenizer once (if not already installed)\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPV2v1jT-tpq",
        "outputId": "d1f0f402-b089-4e7c-ca67-f9bd81228bd0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Function for text preprocessing\n",
        "* to clean the text data by removing unwanted characters, converting text to lowercase, and removing stopwords.\n",
        "* for reducing noise in data; ensuring that the news headlines are cleaned and tokenized, making them ready for sentiment analysis.\n"
      ],
      "metadata": {
        "id": "ODvB6eGE_KCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    # Remove special characters, digits, and extra spaces\n",
        "    # https://regex101.com/\n",
        "\n",
        "    # case sensitive, \"\\W\" matches any non-word character (equivalent to [^a-zA-Z0-9_]) except for line terminators and replace with 'space'\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "\n",
        "    # case sensitive, \"\\s+\"\" matches any whitespace character (equivalent to [\\r\\n\\t\\f\\v ]) any number of times except for line terminators and replace with 'space'\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "XcDU5EJg_IgY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Apply preprocessing to the news headlines\n",
        "* applies the **preprocess_text** function to every row of the **Headline** column and stores the cleaned results in a new column **Cleaned_Headline.**\n",
        "* It cleans the raw headlines and stores the cleaned version in a new column for later use in sentiment analysis.\n"
      ],
      "metadata": {
        "id": "62CfzN0CDn-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df['Cleaned_Headline'] = news_df['Headline'].apply(preprocess_text)\n",
        "print(news_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_oR7pdFD2Hy",
        "outputId": "881ff7e2-a579-4131-f8c6-88a0324fd9ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Headline Cleaned_Headline\n",
            "0                                         BULL'S EYE         bull eye\n",
            "1  \\n                            Must Watch\\n    ...       must watch\n"
          ]
        }
      ]
    }
  ]
}