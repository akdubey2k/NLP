{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction\n",
        "## 1.1 Definition\n",
        "**Lemmatization** is a process in Natural Language Processing (NLP) that reduces words to their base or root form, known as the **lemma**. Unlike **stemming**, which simply removes suffixes from words, lemmatization considers the context and morphological (*relating to the forms of words,*) analysis of words, making it more accurate and linguistically informed."
      ],
      "metadata": {
        "id": "ev29ux4Zhy8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Key features of Lemmatization\n",
        "* **Context-Aware:** Lemmatization uses the context of a word to convert it into its base form. For example, \"better\" is lemmatized as \"good\" because it considers the relationship between words.\n",
        "* **Part-of-Speech Tagging:**It involves tagging words with their correct part of speech (POS) like noun, verb, adjective, etc., which helps in identifying the correct lemma (a heading indicating the subject or argument of a literary composition or annotation) of the word.\n",
        "* **Dictionary-Based Approach:** Lemmatization relies on a dictionary to determine the correct base form of a word, making it more reliable than simple suffix removal.\n",
        "* **Meaning Preservation:** Lemmatization ensures that the root form retains the meaning of the original word.\n",
        "* **Context Awareness:** It takes into account the part of speech (POS) of the word, making it more accurate than stemming.\n",
        "* **Reduced Complexity:** By reducing inflected forms of words to a common base, lemmatization simplifies text data, making it easier to analyze.\n"
      ],
      "metadata": {
        "id": "qjMZQfJ0_WMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import libraries"
      ],
      "metadata": {
        "id": "hX-iInk4AXti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oSCYgs2PhnpM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c644f2-3164-44a0-be42-d718e044d420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# NLTK library works with human language data (text) such as tokenization, part-of-speech tagging, and more.\n",
        "import nltk\n",
        "\n",
        "# WordNetLemmatizer class from the nltk.stem module, which is responsible for lemmatizing words.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# The 'stopwords' module from 'nltk' provides a list of common stopwords for various languages.\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# the pos_tag function, tags words with their part of speech, and word_tokenize, splits text into individual words (tokens).\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Load the necessary 'NLTK' resources"
      ],
      "metadata": {
        "id": "f5mpkDdQBneE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.download('all')\n",
        "\n",
        "# Punkt tokenizer models required for word tokenization.\n",
        "nltk.download('punkt')\n",
        "\n",
        "# WordNet lexical database, provides the data needed for lemmatization.\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# POS tagging model, used to tag words with their parts of speech.\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "HzUOvafCiVp-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ae0fe3-1de1-43a1-eb58-84fc01897c6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load the data\n",
        "Load the dataset either by specifying a URL or by directly writing the entire paragraph here for further data analysis."
      ],
      "metadata": {
        "id": "1FvkzMuf6sYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my first vision is that of freedom. I believe that India got its first vision of this in 1857, when we started the War of Independence. It is this freedom that we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India stands up to the world, no one will respect us. Only strength respects strength. We must be strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career.\"\"\""
      ],
      "metadata": {
        "id": "bdqFkDUEiiRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Pre-processing\n",
        "The preprocessing is essential for **standardizing** and **structuring** text data, enabling more effective and accurate data analysis in subsequent steps."
      ],
      "metadata": {
        "id": "HBlPPY1F7kz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Tokenize the entire paragraph\n",
        "In data pre-processing, the first step is to clean and prepare raw text data for analysis.\n",
        "\n",
        "**Tokenization**, a fundamental technique in **Natural Language Processing (NLP)**, involves breaking down a paragraph into\n",
        "\n",
        "* individual words,\n",
        "* phrases, or\n",
        "* sentences,\n",
        "\n",
        "making the data easier to analyze and process."
      ],
      "metadata": {
        "id": "G5njHv9r-kqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the WordNet Lemmatizer from the NLTK library to reduce words to their base form\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# Split the given paragraph into individual sentences\n",
        "sent_list = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "# Display the list of tokenized sentences\n",
        "sent_list"
      ],
      "metadata": {
        "id": "Pwf7mTWBitaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215602af-016f-49d4-f576-806b7b728d04"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British, the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Setup and use of **stopwords** as per the different language\n",
        "\n",
        "* **Stopwords** are common words in a language that are often filtered out during text processing because they carry little meaningful information. Examples include **\"the,\" \"is,\" \"in,\" \"and,\" etc.**\n",
        "* These words are usually removed in tasks like ***text analysis, information retrieval, and machine learning*** to improve the performance of algorithms by focusing on more relevant words."
      ],
      "metadata": {
        "id": "XPrBFgxz-TNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stopwords.words('german')\n",
        "# stopwords.words('french')\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "CAl6Jqh2-1n5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Text Pre-processing: Tokenization, Stopword Removal, and Lemmatization Using NLTK\""
      ],
      "metadata": {
        "id": "amWIkg_ZSL7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Loop through each sentence in the sentence list using its index\n",
        "for i in range(len(sent_list)):\n",
        "    # Tokenize the sentence at the current index into individual words\n",
        "    word_list = nltk.word_tokenize(sent_list[i])\n",
        "\n",
        "    # Create a list to hold processed words (lemmatized non-stopwords)\n",
        "    processed_words = []\n",
        "\n",
        "    # Iterate over each word in the tokenized word list\n",
        "    for word in word_list:\n",
        "        # Check if the current word is not a stopword\n",
        "        if word.lower() not in set(stopwords.words('english')):\n",
        "            # Lemmatize the current word, reducing it to its base or root form\n",
        "            lemmatized_word = wnl.lemmatize(word)\n",
        "            # Append the lemmatized word to the processed words list\n",
        "            processed_words.append(lemmatized_word)\n",
        "\n",
        "    # Reconstruct the sentence with the processed (lemmatized) words\n",
        "    sent_list[i] = ' '.join(processed_words)\n",
        "\n",
        "    # Print the modified sentence after all words are processed\n",
        "    print(sent_list[i])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "57tzqMfMUUx7",
        "outputId": "d9260100-abec-40fa-cd5a-4beff8b6dd82"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Loop through each sentence in the sentence list using its index\\nfor i in range(len(sent_list)):\\n    # Tokenize the sentence at the current index into individual words\\n    word_list = nltk.word_tokenize(sent_list[i])\\n\\n    # Create a list to hold processed words (lemmatized non-stopwords)\\n    processed_words = []\\n\\n    # Iterate over each word in the tokenized word list\\n    for word in word_list:\\n        # Check if the current word is not a stopword\\n        if word.lower() not in set(stopwords.words('english')):\\n            # Lemmatize the current word, reducing it to its base or root form\\n            lemmatized_word = wnl.lemmatize(word)\\n            # Append the lemmatized word to the processed words list\\n            processed_words.append(lemmatized_word)\\n\\n    # Reconstruct the sentence with the processed (lemmatized) words\\n    sent_list[i] = ' '.join(processed_words)\\n\\n    # Print the modified sentence after all words are processed\\n    print(sent_list[i])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterating through sentences, iterates over each index of 'sent_list', which contains sentences that need processing.\n",
        "for i in range(len(sent_list)):\n",
        "\n",
        "  # tokenizes each sentence at index i into individual words and punctuation.\n",
        "  word_list = nltk.word_tokenize(sent_list[i])\n",
        "\n",
        "  # Create a list to hold processed words (lemmatized non-stopwords)\n",
        "  processed_words = []\n",
        "\n",
        "  # iterate over each word in the tokenized word list\n",
        "  for word in word_list:\n",
        "\n",
        "    # Check if the current word is not a stopword\n",
        "    if word not in set(stopwords.words('english')):\n",
        "\n",
        "      # lemmatizes the non-stopword, reducing it to its base or root form\n",
        "      words = wnl.lemmatize(word)\n",
        "\n",
        "      # Add the lemmatized word to the processed_words list\n",
        "      processed_words.append(words)\n",
        "      # sent_list[i] = ' '.join(words)\n",
        "      # print(sent_list[i])\n",
        "  # Reconstruct the sentence with the processed (lemmatized) words\n",
        "  sent_list[i] = ' '.join(processed_words)\n",
        "\n",
        "  # Print the modified sentence after all words are processed\n",
        "  print(sent_list[i])"
      ],
      "metadata": {
        "id": "ASW3BRO3jQ60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96feab67-c1fb-4f56-839c-5011b245f2c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I three vision India .\n",
            "In 3000 year history , people world come invaded u , captured land , conquered mind .\n",
            "From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .\n",
            "Yet done nation .\n",
            "We conquered anyone .\n",
            "We grabbed land , culture , history tried enforce way life .\n",
            "Why ?\n",
            "Because respect freedom others.That first vision freedom .\n",
            "I believe India got first vision 1857 , started War Independence .\n",
            "It freedom must protect nurture build .\n",
            "If free , one respect u .\n",
            "My second vision India ’ development .\n",
            "For fifty year developing nation .\n",
            "It time see developed nation .\n",
            "We among top 5 nation world term GDP .\n",
            "We 10 percent growth rate area .\n",
            "Our poverty level falling .\n",
            "Our achievement globally recognised today .\n",
            "Yet lack self-confidence see developed nation , self-reliant self-assured .\n",
            "Isn ’ incorrect ?\n",
            "I third vision .\n",
            "India must stand world .\n",
            "Because I believe unless India stand world , one respect u .\n",
            "Only strength respect strength .\n",
            "We must strong military power also economic power .\n",
            "Both must go hand-in-hand .\n",
            "My good fortune worked three great mind .\n",
            "Dr. Vikram Sarabhai Dept .\n",
            "space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .\n",
            "I lucky worked three closely consider great opportunity life .\n",
            "I see four milestone career .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_list = nltk.sent_tokenize(paragraph)\n",
        "\n",
        "for i in range(len(sent_list)):\n",
        "  word_list = nltk.word_tokenize(sent_list[i]) # list comprehension, has to go in list only\n",
        "  words = [wnl.lemmatize(word) for word in word_list if word not in set(stopwords.words('english'))]\n",
        "  sent_list[i] = ' '.join(words)\n",
        "  print(sent_list[i])"
      ],
      "metadata": {
        "id": "wsu5JYqtkhCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696cbfb3-a565-476a-9063-56a9f588f22e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I three vision India .\n",
            "In 3000 year history , people world come invaded u , captured land , conquered mind .\n",
            "From Alexander onwards , Greeks , Turks , Moguls , Portuguese , British , French , Dutch , came looted u , took .\n",
            "Yet done nation .\n",
            "We conquered anyone .\n",
            "We grabbed land , culture , history tried enforce way life .\n",
            "Why ?\n",
            "Because respect freedom others.That first vision freedom .\n",
            "I believe India got first vision 1857 , started War Independence .\n",
            "It freedom must protect nurture build .\n",
            "If free , one respect u .\n",
            "My second vision India ’ development .\n",
            "For fifty year developing nation .\n",
            "It time see developed nation .\n",
            "We among top 5 nation world term GDP .\n",
            "We 10 percent growth rate area .\n",
            "Our poverty level falling .\n",
            "Our achievement globally recognised today .\n",
            "Yet lack self-confidence see developed nation , self-reliant self-assured .\n",
            "Isn ’ incorrect ?\n",
            "I third vision .\n",
            "India must stand world .\n",
            "Because I believe unless India stand world , one respect u .\n",
            "Only strength respect strength .\n",
            "We must strong military power also economic power .\n",
            "Both must go hand-in-hand .\n",
            "My good fortune worked three great mind .\n",
            "Dr. Vikram Sarabhai Dept .\n",
            "space , Professor Satish Dhawan , succeeded Dr. Brahm Prakash , father nuclear material .\n",
            "I lucky worked three closely consider great opportunity life .\n",
            "I see four milestone career .\n"
          ]
        }
      ]
    }
  ]
}