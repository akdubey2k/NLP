{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [Natural Language Processing | Text Preprocessing | Spacy vs NLTK](https://medium.com/nerd-for-tech/natural-language-processing-text-preprocessing-spacy-vs-nltk-b70b734f5560#)"
      ],
      "metadata": {
        "id": "c6wvke-crCiS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF_3Pq0CrB5v"
      },
      "outputs": [],
      "source": [
        "Examples\n",
        "1. Tokenization:\n",
        "Tokenization is the process of splitting text into individual tokens (words, punctuation, etc.).\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"SpaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "['SpaCy', 'is', 'a', 'fast', 'and', 'robust', 'library', 'for', 'NLP', '.']\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.']\n",
        "2. Part-of-Speech (POS) Tagging:\n",
        "POS tagging assigns parts of speech to each token.\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"SpaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(pos_tags)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('SpaCy', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('fast', 'ADJ'), ('and', 'CCONJ'), ('robust', 'ADJ'), ('library', 'NOUN'), ('for', 'ADP'), ('NLP', 'PROPN'), ('.', 'PUNCT')]\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenization and POS tagging\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n",
        "3. Named Entity Recognition (NER):\n",
        "NER identifies and classifies named entities in text.\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(entities)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Tokenization, POS tagging, and NER\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(entities)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "(S\n",
        "  (ORGANIZATION Apple/NNP)\n",
        "  is/VBZ\n",
        "  looking/VBG\n",
        "  at/IN\n",
        "  buying/VBG\n",
        "  (GPE U.K./NNP)\n",
        "  startup/NN\n",
        "  for/IN\n",
        "  $/$\n",
        "  1/CD\n",
        "  billion/CD\n",
        "  ./.)\n",
        "Summary\n",
        "SpaCy is preferred for production applications due to its speed and modern NLP capabilities.\n",
        "NLTK is great for educational purposes and research, providing a wide range of linguistic data and tools.\n",
        "Each library has its strengths and can be chosen based on the specific needs of the project.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check im"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples\n",
        "1. Tokenization:\n",
        "Tokenization is the process of splitting text into individual tokens (words, punctuation, etc.).\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"SpaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "['SpaCy', 'is', 'a', 'fast', 'and', 'robust', 'library', 'for', 'NLP', '.']\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.']\n",
        "2. Part-of-Speech (POS) Tagging:\n",
        "POS tagging assigns parts of speech to each token.\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"SpaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(pos_tags)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('SpaCy', 'PROPN'), ('is', 'AUX'), ('a', 'DET'), ('fast', 'ADJ'), ('and', 'CCONJ'), ('robust', 'ADJ'), ('library', 'NOUN'), ('for', 'ADP'), ('NLP', 'PROPN'), ('.', 'PUNCT')]\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenization and POS tagging\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n",
        "3. Named Entity Recognition (NER):\n",
        "NER identifies and classifies named entities in text.\n",
        "\n",
        "SpaCy:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(entities)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n",
        "NLTK:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Tokenization, POS tagging, and NER\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(entities)\n",
        "Output:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "(S\n",
        "  (ORGANIZATION Apple/NNP)\n",
        "  is/VBZ\n",
        "  looking/VBG\n",
        "  at/IN\n",
        "  buying/VBG\n",
        "  (GPE U.K./NNP)\n",
        "  startup/NN\n",
        "  for/IN\n",
        "  $/$\n",
        "  1/CD\n",
        "  billion/CD\n",
        "  ./.)\n",
        "Summary\n",
        "SpaCy is preferred for production applications due to its speed and modern NLP capabilities.\n",
        "NLTK is great for educational purposes and research, providing a wide range of linguistic data and tools.\n",
        "Each library has its strengths and can be chosen based on the specific needs of the project.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT can make mistakes. Check im"
      ],
      "metadata": {
        "id": "7kfKniv9UCRY"
      }
    }
  ]
}