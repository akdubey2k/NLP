{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Description\n",
        "* **NLTK** is a broad and general-purpose **NLP** library that provides a wide range of tools and algorithms for tasks like:-\n",
        " * tokenization,\n",
        " * stemming,\n",
        " * tagging,\n",
        " * parsing, and\n",
        " * sentiment analysis.\n",
        "* **spaCy,** on the other hand, is *more focused on industrial-strength NLP tasks,* such as\n",
        " * named entity recognition,\n",
        " * dependency parsing, and\n",
        " * text classification.\n",
        "\n",
        "  **spaCy,** *is designed for high-performance, production-ready applications.*"
      ],
      "metadata": {
        "id": "c6wvke-crCiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples\n",
        "## 1. Tokenization:\n",
        "**Tokenization** is the process of *splitting text into individual tokens (words, punctuation, etc.).*\n",
        "\n",
        "### *spaCy implementation*"
      ],
      "metadata": {
        "id": "yCNcmZsZj-K-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oF_3Pq0CrB5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1142fd3a-fcb7-492f-a118-3397fc5408cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spaCy', 'is', 'a', 'fast', 'and', 'robust', 'library', 'for', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"spaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# Tokenization\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *NLTK implementation*"
      ],
      "metadata": {
        "id": "MeRSKIQEkv9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenization\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjBFbVGZk_e9",
        "outputId": "1f71e622-9d57-4fe5-910d-0783d18b866e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'NLP', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Part-of-Speech (POS) Tagging:\n",
        "**POS** tagging assigns *parts of speech* to each token.\n",
        "\n",
        "### *spaCy implementation*"
      ],
      "metadata": {
        "id": "SJUZ3XUPlKVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"spaCy is a fast and robust library for NLP.\")\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn85NLtZlqJV",
        "outputId": "4246d396-4f4f-4efb-fe61-12d352bf7922"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('spaCy', 'INTJ'), ('is', 'AUX'), ('a', 'DET'), ('fast', 'ADJ'), ('and', 'CCONJ'), ('robust', 'ADJ'), ('library', 'NOUN'), ('for', 'ADP'), ('NLP', 'PROPN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *NLTK implementation*"
      ],
      "metadata": {
        "id": "MuP9OXWpl6t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Tokenization and POS tagging\n",
        "text = \"NLTK is a powerful library for NLP.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x57z-W2ul9Qs",
        "outputId": "9fa50ab6-2052-4a84-b822-1789f8b773c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Named Entity Recognition (NER)\n",
        "**NER** *identifies and classifies named entities in text.*\n",
        "\n",
        "### *spaCy implementation*\n",
        "\n"
      ],
      "metadata": {
        "id": "3g1NlUm2mGTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKFEBT7VmbHd",
        "outputId": "b61a5405-a99a-4849-909e-42ada7742aa0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *NLTK implementation*"
      ],
      "metadata": {
        "id": "N9IJpLBemrlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download the necessary NLTK data files (if not already downloaded)\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Tokenization, POS tagging, and NER\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "entities = ne_chunk(pos_tags)\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Eps5LpmkKU",
        "outputId": "a86c184e-3bac-4e7c-e9f0-9357b8151eec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Apple/NNP)\n",
            "  is/VBZ\n",
            "  looking/VBG\n",
            "  at/IN\n",
            "  buying/VBG\n",
            "  U.K./NNP\n",
            "  startup/NN\n",
            "  for/IN\n",
            "  $/$\n",
            "  1/CD\n",
            "  billion/CD\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "* **spaCy** is preferred for production applications due to its speed and modern NLP capabilities.\n",
        "* **NLTK** is great for educational purposes and research, providing a wide range of linguistic data and tools.\n",
        "* Each library has its strengths and can be chosen based on the specific needs of the project."
      ],
      "metadata": {
        "id": "Ov_LpZ1lm4Vp"
      }
    }
  ]
}